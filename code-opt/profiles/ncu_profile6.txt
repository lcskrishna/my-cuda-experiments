==PROF== Connected to process 8045 (/workspace/my-cuda-experiments/code-opt/bin/interview_problem.exe)
==PROF== Profiling "kernel_fill_apply_v2_dim2" - 0: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim2" - 1: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim2" - 2: 0%....50%....100% - 19 passes
==PROF== Profiling "reduce_dim_1" - 3: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_fill_apply_v2_dim1" - 4: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 5: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_broadcast_apply" - 6: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim2" - 7: 0%....50%....100% - 19 passes
==PROF== Profiling "reduce_dim_1" - 8: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 9: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_fill_apply_v2_dim1" - 10: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_broadcast_apply" - 11: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 12: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 13: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_broadcast_apply" - 14: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_fill_apply_v2_dim2" - 15: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim2" - 16: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim2" - 17: 0%....50%....100% - 19 passes
==PROF== Profiling "reduce_dim_1" - 18: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_fill_apply_v2_dim1" - 19: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 20: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_broadcast_apply" - 21: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim2" - 22: 0%....50%....100% - 19 passes
==PROF== Profiling "reduce_dim_1" - 23: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 24: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_fill_apply_v2_dim1" - 25: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_broadcast_apply" - 26: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 27: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 28: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_broadcast_apply" - 29: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_fill_apply_v2_dim2" - 30: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim2" - 31: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim2" - 32: 0%....50%....100% - 19 passes
==PROF== Profiling "reduce_dim_1" - 33: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_fill_apply_v2_dim1" - 34: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 35: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_broadcast_apply" - 36: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim2" - 37: 0%....50%....100% - 19 passes
==PROF== Profiling "reduce_dim_1" - 38: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 39: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_fill_apply_v2_dim1" - 40: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_broadcast_apply" - 41: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 42: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 43: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_broadcast_apply" - 44: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_fill_apply_v2_dim2" - 45: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim2" - 46: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim2" - 47: 0%....50%....100% - 19 passes
==PROF== Profiling "reduce_dim_1" - 48: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_fill_apply_v2_dim1" - 49: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 50: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_broadcast_apply" - 51: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim2" - 52: 0%....50%....100% - 19 passes
==PROF== Profiling "reduce_dim_1" - 53: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 54: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_fill_apply_v2_dim1" - 55: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_broadcast_apply" - 56: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 57: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 58: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_broadcast_apply" - 59: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_fill_apply_v2_dim2" - 60: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim2" - 61: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim2" - 62: 0%....50%....100% - 19 passes
==PROF== Profiling "reduce_dim_1" - 63: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_fill_apply_v2_dim1" - 64: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 65: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_broadcast_apply" - 66: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim2" - 67: 0%....50%....100% - 19 passes
==PROF== Profiling "reduce_dim_1" - 68: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 69: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_fill_apply_v2_dim1" - 70: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_broadcast_apply" - 71: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 72: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 73: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_broadcast_apply" - 74: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_fill_apply_v2_dim2" - 75: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim2" - 76: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim2" - 77: 0%....50%....100% - 19 passes
==PROF== Profiling "reduce_dim_1" - 78: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_fill_apply_v2_dim1" - 79: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 80: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_broadcast_apply" - 81: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim2" - 82: 0%....50%....100% - 19 passes
==PROF== Profiling "reduce_dim_1" - 83: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 84: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_fill_apply_v2_dim1" - 85: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_broadcast_apply" - 86: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 87: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 88: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_broadcast_apply" - 89: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_fill_apply_v2_dim2" - 90: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim2" - 91: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim2" - 92: 0%....50%....100% - 19 passes
==PROF== Profiling "reduce_dim_1" - 93: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_fill_apply_v2_dim1" - 94: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 95: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_broadcast_apply" - 96: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim2" - 97: 0%....50%....100% - 19 passes
==PROF== Profiling "reduce_dim_1" - 98: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 99: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_fill_apply_v2_dim1" - 100: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_broadcast_apply" - 101: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 102: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 103: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_broadcast_apply" - 104: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_fill_apply_v2_dim2" - 105: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim2" - 106: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim2" - 107: 0%....50%....100% - 19 passes
==PROF== Profiling "reduce_dim_1" - 108: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_fill_apply_v2_dim1" - 109: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 110: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_broadcast_apply" - 111: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim2" - 112: 0%....50%....100% - 19 passes
==PROF== Profiling "reduce_dim_1" - 113: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 114: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_fill_apply_v2_dim1" - 115: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_broadcast_apply" - 116: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 117: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_pointwise_apply_v2_dim1" - 118: 0%....50%....100% - 19 passes
==PROF== Profiling "kernel_broadcast_apply" - 119: 0%....50%....100% - 19 passes
Finished in 30166.6 ms.
==PROF== Disconnected from process 8045
[8045] interview_problem.exe@127.0.0.1
  void kernel_fill_apply_v2_dim2<(int)2>(device_tensor<T1>, float), 2022-Oct-10 02:10:06, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         781.15
    SM Frequency                                                             cycle/nsecond                           1.16
    Elapsed Cycles                                                                   cycle                          23912
    Memory [%]                                                                           %                          78.73
    DRAM Throughput                                                                      %                          78.73
    Duration                                                                       usecond                          20.54
    L1/TEX Cache Throughput                                                              %                          54.90
    L2 Cache Throughput                                                                  %                          36.19
    SM Active Cycles                                                                 cycle                       20608.16
    Compute (SM) [%]                                                                     %                          27.44
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          52.21
    Achieved Active Warps Per SM                                                      warp                          33.41
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (52.2%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim2<div_op, (int)2>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:06, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         820.52
    SM Frequency                                                             cycle/nsecond                           1.22
    Elapsed Cycles                                                                   cycle                          76935
    Memory [%]                                                                           %                          89.40
    DRAM Throughput                                                                      %                          89.40
    Duration                                                                       usecond                          62.85
    L1/TEX Cache Throughput                                                              %                          25.57
    L2 Cache Throughput                                                                  %                          33.51
    SM Active Cycles                                                                 cycle                       73968.76
    Compute (SM) [%]                                                                     %                          15.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          85.43
    Achieved Active Warps Per SM                                                      warp                          54.68
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (85.4%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim2<sinh_op, (int)2>(device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:06, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         815.64
    SM Frequency                                                             cycle/nsecond                           1.22
    Elapsed Cycles                                                                   cycle                          54281
    Memory [%]                                                                           %                          81.55
    DRAM Throughput                                                                      %                          81.55
    Duration                                                                       usecond                          44.61
    L1/TEX Cache Throughput                                                              %                          30.21
    L2 Cache Throughput                                                                  %                          31.69
    SM Active Cycles                                                                 cycle                       51034.76
    Compute (SM) [%]                                                                     %                          22.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          80.84
    Achieved Active Warps Per SM                                                      warp                          51.74
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (80.8%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void reduce_dim_1<add_op>(device_tensor<(int)1>, device_tensor<(int)2>), 2022-Oct-10 02:10:06, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         876.63
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       10906719
    Memory [%]                                                                           %                           0.48
    DRAM Throughput                                                                      %                           0.23
    Duration                                                                       msecond                           8.32
    L1/TEX Cache Throughput                                                              %                          38.44
    L2 Cache Throughput                                                                  %                           0.08
    SM Active Cycles                                                                 cycle                      136395.91
    Compute (SM) [%]                                                                     %                           0.02
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             24
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             84
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                           1.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_fill_apply_v2_dim1<(int)1>(device_tensor<T1>, float), 2022-Oct-10 02:10:06, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         582.42
    SM Frequency                                                             cycle/usecond                         868.36
    Elapsed Cycles                                                                   cycle                           2530
    Memory [%]                                                                           %                           0.61
    DRAM Throughput                                                                      %                           0.01
    Duration                                                                       usecond                           2.91
    L1/TEX Cache Throughput                                                              %                           1.25
    L2 Cache Throughput                                                                  %                           0.61
    SM Active Cycles                                                                 cycle                         674.25
    Compute (SM) [%]                                                                     %                           0.24
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.29
    Achieved Active Warps Per SM                                                      warp                           1.46
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.3%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<div_op, (int)1>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:06, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         551.72
    SM Frequency                                                             cycle/usecond                         826.37
    Elapsed Cycles                                                                   cycle                           3069
    Memory [%]                                                                           %                           1.56
    DRAM Throughput                                                                      %                           1.56
    Duration                                                                       usecond                           3.71
    L1/TEX Cache Throughput                                                              %                           1.03
    L2 Cache Throughput                                                                  %                           0.91
    SM Active Cycles                                                                 cycle                        1245.47
    Compute (SM) [%]                                                                     %                           0.39
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.39
    Achieved Active Warps Per SM                                                      warp                           1.53
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_broadcast_apply<sub_op>(device_tensor<(int)2>, device_tensor<(int)2>, device_tensor<(int)1>), 2022-Oct-10 02:10:07, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.43
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       24213757
    Memory [%]                                                                           %                           0.60
    DRAM Throughput                                                                      %                           0.21
    Duration                                                                       msecond                          18.45
    L1/TEX Cache Throughput                                                              %                          46.00
    L2 Cache Throughput                                                                  %                           0.60
    SM Active Cycles                                                                 cycle                      302690.30
    Compute (SM) [%]                                                                     %                           0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim2<square_op, (int)2>(device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:07, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         804.33
    SM Frequency                                                             cycle/nsecond                           1.20
    Elapsed Cycles                                                                   cycle                          54115
    Memory [%]                                                                           %                          81.86
    DRAM Throughput                                                                      %                          81.86
    Duration                                                                       usecond                          45.06
    L1/TEX Cache Throughput                                                              %                          30.30
    L2 Cache Throughput                                                                  %                          31.79
    SM Active Cycles                                                                 cycle                       50497.35
    Compute (SM) [%]                                                                     %                          15.15
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          80.98
    Achieved Active Warps Per SM                                                      warp                          51.83
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (81.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void reduce_dim_1<add_op>(device_tensor<(int)1>, device_tensor<(int)2>), 2022-Oct-10 02:10:07, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         876.67
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       10908773
    Memory [%]                                                                           %                           0.48
    DRAM Throughput                                                                      %                           0.22
    Duration                                                                       msecond                           8.32
    L1/TEX Cache Throughput                                                              %                          38.46
    L2 Cache Throughput                                                                  %                           0.08
    SM Active Cycles                                                                 cycle                      136333.23
    Compute (SM) [%]                                                                     %                           0.02
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             24
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             84
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<div_op, (int)1>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:08, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         561.95
    SM Frequency                                                             cycle/usecond                         844.81
    Elapsed Cycles                                                                   cycle                           3056
    Memory [%]                                                                           %                           1.58
    DRAM Throughput                                                                      %                           1.58
    Duration                                                                       usecond                           3.62
    L1/TEX Cache Throughput                                                              %                           0.89
    L2 Cache Throughput                                                                  %                           0.92
    SM Active Cycles                                                                 cycle                        1434.17
    Compute (SM) [%]                                                                     %                           0.39
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.40
    Achieved Active Warps Per SM                                                      warp                           1.53
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_fill_apply_v2_dim1<(int)1>(device_tensor<T1>, float), 2022-Oct-10 02:10:08, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         589.89
    SM Frequency                                                             cycle/usecond                         891.74
    Elapsed Cycles                                                                   cycle                           2541
    Memory [%]                                                                           %                           0.61
    DRAM Throughput                                                                      %                           0.01
    Duration                                                                       usecond                           2.85
    L1/TEX Cache Throughput                                                              %                           1.25
    L2 Cache Throughput                                                                  %                           0.61
    SM Active Cycles                                                                 cycle                         673.35
    Compute (SM) [%]                                                                     %                           0.24
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.29
    Achieved Active Warps Per SM                                                      warp                           1.46
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.3%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_broadcast_apply<sub_op>(device_tensor<(int)2>, device_tensor<(int)2>, device_tensor<(int)1>), 2022-Oct-10 02:10:08, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.13
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       24220225
    Memory [%]                                                                           %                           0.60
    DRAM Throughput                                                                      %                           0.21
    Duration                                                                       msecond                          18.46
    L1/TEX Cache Throughput                                                              %                          46.00
    L2 Cache Throughput                                                                  %                           0.60
    SM Active Cycles                                                                 cycle                      302706.42
    Compute (SM) [%]                                                                     %                           0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<add_op, (int)1>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:08, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         567.48
    SM Frequency                                                             cycle/usecond                         847.25
    Elapsed Cycles                                                                   cycle                           3064
    Memory [%]                                                                           %                           1.56
    DRAM Throughput                                                                      %                           1.56
    Duration                                                                       usecond                           3.62
    L1/TEX Cache Throughput                                                              %                           1.05
    L2 Cache Throughput                                                                  %                           0.91
    SM Active Cycles                                                                 cycle                        1213.47
    Compute (SM) [%]                                                                     %                           0.26
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.38
    Achieved Active Warps Per SM                                                      warp                           1.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<square_root_op, (int)1>(device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:09, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         557.52
    SM Frequency                                                             cycle/usecond                         825.31
    Elapsed Cycles                                                                   cycle                           2986
    Memory [%]                                                                           %                           0.80
    DRAM Throughput                                                                      %                           0.80
    Duration                                                                       usecond                           3.62
    L1/TEX Cache Throughput                                                              %                           0.80
    L2 Cache Throughput                                                                  %                           0.66
    SM Active Cycles                                                                 cycle                        1252.96
    Compute (SM) [%]                                                                     %                           0.36
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.36
    Achieved Active Warps Per SM                                                      warp                           1.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_broadcast_apply<div_op>(device_tensor<(int)2>, device_tensor<(int)2>, device_tensor<(int)1>), 2022-Oct-10 02:10:09, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.40
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       28912606
    Memory [%]                                                                           %                           0.50
    DRAM Throughput                                                                      %                           0.18
    Duration                                                                       msecond                          22.03
    L1/TEX Cache Throughput                                                              %                          38.53
    L2 Cache Throughput                                                                  %                           0.50
    SM Active Cycles                                                                 cycle                      361389.78
    Compute (SM) [%]                                                                     %                           0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             35
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             48
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_fill_apply_v2_dim2<(int)2>(device_tensor<T1>, float), 2022-Oct-10 02:10:09, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         780.86
    SM Frequency                                                             cycle/nsecond                           1.16
    Elapsed Cycles                                                                   cycle                          23814
    Memory [%]                                                                           %                          78.92
    DRAM Throughput                                                                      %                          78.92
    Duration                                                                       usecond                          20.48
    L1/TEX Cache Throughput                                                              %                          55.12
    L2 Cache Throughput                                                                  %                          36.36
    SM Active Cycles                                                                 cycle                       20638.74
    Compute (SM) [%]                                                                     %                          27.55
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          52.10
    Achieved Active Warps Per SM                                                      warp                          33.34
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (52.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim2<div_op, (int)2>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:09, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         829.38
    SM Frequency                                                             cycle/nsecond                           1.24
    Elapsed Cycles                                                                   cycle                          76834
    Memory [%]                                                                           %                          89.56
    DRAM Throughput                                                                      %                          89.56
    Duration                                                                       usecond                          62.08
    L1/TEX Cache Throughput                                                              %                          25.61
    L2 Cache Throughput                                                                  %                          33.56
    SM Active Cycles                                                                 cycle                       73654.49
    Compute (SM) [%]                                                                     %                          15.54
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          84.94
    Achieved Active Warps Per SM                                                      warp                          54.36
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (84.9%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim2<sinh_op, (int)2>(device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:10, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         813.62
    SM Frequency                                                             cycle/nsecond                           1.21
    Elapsed Cycles                                                                   cycle                          54401
    Memory [%]                                                                           %                          81.50
    DRAM Throughput                                                                      %                          81.50
    Duration                                                                       usecond                          44.77
    L1/TEX Cache Throughput                                                              %                          30.14
    L2 Cache Throughput                                                                  %                          31.61
    SM Active Cycles                                                                 cycle                       51372.21
    Compute (SM) [%]                                                                     %                          21.95
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          80.98
    Achieved Active Warps Per SM                                                      warp                          51.83
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (81.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void reduce_dim_1<add_op>(device_tensor<(int)1>, device_tensor<(int)2>), 2022-Oct-10 02:10:10, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         876.75
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       10909761
    Memory [%]                                                                           %                           0.48
    DRAM Throughput                                                                      %                           0.22
    Duration                                                                       msecond                           8.32
    L1/TEX Cache Throughput                                                              %                          38.46
    L2 Cache Throughput                                                                  %                           0.08
    SM Active Cycles                                                                 cycle                      136322.67
    Compute (SM) [%]                                                                     %                           0.02
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             24
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             84
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_fill_apply_v2_dim1<(int)1>(device_tensor<T1>, float), 2022-Oct-10 02:10:10, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         644.44
    SM Frequency                                                             cycle/usecond                         955.90
    Elapsed Cycles                                                                   cycle                           2754
    Memory [%]                                                                           %                           0.56
    DRAM Throughput                                                                      %                           0.01
    Duration                                                                       usecond                           2.88
    L1/TEX Cache Throughput                                                              %                           1.24
    L2 Cache Throughput                                                                  %                           0.56
    SM Active Cycles                                                                 cycle                         679.86
    Compute (SM) [%]                                                                     %                           0.22
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.29
    Achieved Active Warps Per SM                                                      warp                           1.46
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.3%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<div_op, (int)1>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:10, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         573.91
    SM Frequency                                                             cycle/usecond                         857.88
    Elapsed Cycles                                                                   cycle                           3159
    Memory [%]                                                                           %                           1.52
    DRAM Throughput                                                                      %                           1.52
    Duration                                                                       usecond                           3.68
    L1/TEX Cache Throughput                                                              %                           1.03
    L2 Cache Throughput                                                                  %                           0.89
    SM Active Cycles                                                                 cycle                        1248.69
    Compute (SM) [%]                                                                     %                           0.38
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.39
    Achieved Active Warps Per SM                                                      warp                           1.53
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_broadcast_apply<sub_op>(device_tensor<(int)2>, device_tensor<(int)2>, device_tensor<(int)1>), 2022-Oct-10 02:10:11, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         876.95
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       24220155
    Memory [%]                                                                           %                           0.60
    DRAM Throughput                                                                      %                           0.21
    Duration                                                                       msecond                          18.46
    L1/TEX Cache Throughput                                                              %                          45.98
    L2 Cache Throughput                                                                  %                           0.60
    SM Active Cycles                                                                 cycle                      302808.99
    Compute (SM) [%]                                                                     %                           0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                           1.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim2<square_op, (int)2>(device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:11, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         807.83
    SM Frequency                                                             cycle/nsecond                           1.21
    Elapsed Cycles                                                                   cycle                          53766
    Memory [%]                                                                           %                          82.41
    DRAM Throughput                                                                      %                          82.41
    Duration                                                                       usecond                          44.54
    L1/TEX Cache Throughput                                                              %                          30.50
    L2 Cache Throughput                                                                  %                          31.99
    SM Active Cycles                                                                 cycle                       50468.40
    Compute (SM) [%]                                                                     %                          15.25
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          81.06
    Achieved Active Warps Per SM                                                      warp                          51.88
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (81.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void reduce_dim_1<add_op>(device_tensor<(int)1>, device_tensor<(int)2>), 2022-Oct-10 02:10:11, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         876.93
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       10911546
    Memory [%]                                                                           %                           0.48
    DRAM Throughput                                                                      %                           0.22
    Duration                                                                       msecond                           8.32
    L1/TEX Cache Throughput                                                              %                          38.46
    L2 Cache Throughput                                                                  %                           0.08
    SM Active Cycles                                                                 cycle                      136346.19
    Compute (SM) [%]                                                                     %                           0.02
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             24
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             84
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                           1.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<div_op, (int)1>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:11, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         567.57
    SM Frequency                                                             cycle/usecond                         852.57
    Elapsed Cycles                                                                   cycle                           3029
    Memory [%]                                                                           %                           1.59
    DRAM Throughput                                                                      %                           1.59
    Duration                                                                       usecond                           3.55
    L1/TEX Cache Throughput                                                              %                           0.97
    L2 Cache Throughput                                                                  %                           0.92
    SM Active Cycles                                                                 cycle                           1319
    Compute (SM) [%]                                                                     %                           0.40
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.38
    Achieved Active Warps Per SM                                                      warp                           1.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_fill_apply_v2_dim1<(int)1>(device_tensor<T1>, float), 2022-Oct-10 02:10:11, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         589.89
    SM Frequency                                                             cycle/usecond                         887.76
    Elapsed Cycles                                                                   cycle                           2529
    Memory [%]                                                                           %                           0.61
    DRAM Throughput                                                                      %                           0.01
    Duration                                                                       usecond                           2.85
    L1/TEX Cache Throughput                                                              %                           1.04
    L2 Cache Throughput                                                                  %                           0.61
    SM Active Cycles                                                                 cycle                         809.94
    Compute (SM) [%]                                                                     %                           0.24
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.32
    Achieved Active Warps Per SM                                                      warp                           1.49
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.3%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_broadcast_apply<sub_op>(device_tensor<(int)2>, device_tensor<(int)2>, device_tensor<(int)1>), 2022-Oct-10 02:10:12, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.39
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       24220638
    Memory [%]                                                                           %                           0.60
    DRAM Throughput                                                                      %                           0.21
    Duration                                                                       msecond                          18.45
    L1/TEX Cache Throughput                                                              %                          45.99
    L2 Cache Throughput                                                                  %                           0.60
    SM Active Cycles                                                                 cycle                      302719.41
    Compute (SM) [%]                                                                     %                           0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                           1.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<add_op, (int)1>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:12, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         563.06
    SM Frequency                                                             cycle/usecond                         843.23
    Elapsed Cycles                                                                   cycle                           2997
    Memory [%]                                                                           %                           1.60
    DRAM Throughput                                                                      %                           1.60
    Duration                                                                       usecond                           3.55
    L1/TEX Cache Throughput                                                              %                           1.05
    L2 Cache Throughput                                                                  %                           0.93
    SM Active Cycles                                                                 cycle                        1213.74
    Compute (SM) [%]                                                                     %                           0.27
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.37
    Achieved Active Warps Per SM                                                      warp                           1.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<square_root_op, (int)1>(device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:12, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         551.72
    SM Frequency                                                             cycle/usecond                         831.31
    Elapsed Cycles                                                                   cycle                           3087
    Memory [%]                                                                           %                           0.78
    DRAM Throughput                                                                      %                           0.78
    Duration                                                                       usecond                           3.71
    L1/TEX Cache Throughput                                                              %                           0.86
    L2 Cache Throughput                                                                  %                           0.64
    SM Active Cycles                                                                 cycle                        1157.94
    Compute (SM) [%]                                                                     %                           0.35
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.37
    Achieved Active Warps Per SM                                                      warp                           1.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_broadcast_apply<div_op>(device_tensor<(int)2>, device_tensor<(int)2>, device_tensor<(int)1>), 2022-Oct-10 02:10:13, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.56
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       28925748
    Memory [%]                                                                           %                           0.50
    DRAM Throughput                                                                      %                           0.18
    Duration                                                                       msecond                          22.03
    L1/TEX Cache Throughput                                                              %                          38.50
    L2 Cache Throughput                                                                  %                           0.50
    SM Active Cycles                                                                 cycle                      361636.08
    Compute (SM) [%]                                                                     %                           0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             35
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             48
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_fill_apply_v2_dim2<(int)2>(device_tensor<T1>, float), 2022-Oct-10 02:10:13, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         772.09
    SM Frequency                                                             cycle/nsecond                           1.15
    Elapsed Cycles                                                                   cycle                          23772
    Memory [%]                                                                           %                          79.34
    DRAM Throughput                                                                      %                          79.34
    Duration                                                                       usecond                          20.64
    L1/TEX Cache Throughput                                                              %                          55.22
    L2 Cache Throughput                                                                  %                          36.41
    SM Active Cycles                                                                 cycle                       20794.25
    Compute (SM) [%]                                                                     %                          27.60
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          52.30
    Achieved Active Warps Per SM                                                      warp                          33.48
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (52.3%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim2<div_op, (int)2>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:13, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         827.16
    SM Frequency                                                             cycle/nsecond                           1.23
    Elapsed Cycles                                                                   cycle                          76778
    Memory [%]                                                                           %                          89.59
    DRAM Throughput                                                                      %                          89.59
    Duration                                                                       usecond                          62.21
    L1/TEX Cache Throughput                                                              %                          25.63
    L2 Cache Throughput                                                                  %                          33.58
    SM Active Cycles                                                                 cycle                       73882.65
    Compute (SM) [%]                                                                     %                          15.56
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          85.21
    Achieved Active Warps Per SM                                                      warp                          54.54
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (85.2%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim2<sinh_op, (int)2>(device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:13, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         816.56
    SM Frequency                                                             cycle/nsecond                           1.22
    Elapsed Cycles                                                                   cycle                          54641
    Memory [%]                                                                           %                          81.05
    DRAM Throughput                                                                      %                          81.05
    Duration                                                                       usecond                          44.83
    L1/TEX Cache Throughput                                                              %                          30.02
    L2 Cache Throughput                                                                  %                          31.48
    SM Active Cycles                                                                 cycle                       51576.09
    Compute (SM) [%]                                                                     %                          23.20
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          79.50
    Achieved Active Warps Per SM                                                      warp                          50.88
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (79.5%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void reduce_dim_1<add_op>(device_tensor<(int)1>, device_tensor<(int)2>), 2022-Oct-10 02:10:14, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         876.48
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       10906238
    Memory [%]                                                                           %                           0.48
    DRAM Throughput                                                                      %                           0.22
    Duration                                                                       msecond                           8.32
    L1/TEX Cache Throughput                                                              %                          38.46
    L2 Cache Throughput                                                                  %                           0.08
    SM Active Cycles                                                                 cycle                      136314.50
    Compute (SM) [%]                                                                     %                           0.02
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             24
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             84
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_fill_apply_v2_dim1<(int)1>(device_tensor<T1>, float), 2022-Oct-10 02:10:14, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         601.12
    SM Frequency                                                             cycle/usecond                         891.27
    Elapsed Cycles                                                                   cycle                           2539
    Memory [%]                                                                           %                           0.61
    DRAM Throughput                                                                      %                           0.01
    Duration                                                                       usecond                           2.85
    L1/TEX Cache Throughput                                                              %                           1.25
    L2 Cache Throughput                                                                  %                           0.61
    SM Active Cycles                                                                 cycle                         670.71
    Compute (SM) [%]                                                                     %                           0.24
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.29
    Achieved Active Warps Per SM                                                      warp                           1.47
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.3%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<div_op, (int)1>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:14, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         590.71
    SM Frequency                                                             cycle/usecond                         878.27
    Elapsed Cycles                                                                   cycle                           3177
    Memory [%]                                                                           %                           1.50
    DRAM Throughput                                                                      %                           1.50
    Duration                                                                       usecond                           3.62
    L1/TEX Cache Throughput                                                              %                           1.03
    L2 Cache Throughput                                                                  %                           0.88
    SM Active Cycles                                                                 cycle                        1246.75
    Compute (SM) [%]                                                                     %                           0.38
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.38
    Achieved Active Warps Per SM                                                      warp                           1.53
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_broadcast_apply<sub_op>(device_tensor<(int)2>, device_tensor<(int)2>, device_tensor<(int)1>), 2022-Oct-10 02:10:14, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.18
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       24223316
    Memory [%]                                                                           %                           0.60
    DRAM Throughput                                                                      %                           0.21
    Duration                                                                       msecond                          18.46
    L1/TEX Cache Throughput                                                              %                          45.99
    L2 Cache Throughput                                                                  %                           0.60
    SM Active Cycles                                                                 cycle                      302738.45
    Compute (SM) [%]                                                                     %                           0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim2<square_op, (int)2>(device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:15, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         807.75
    SM Frequency                                                             cycle/nsecond                           1.20
    Elapsed Cycles                                                                   cycle                          53762
    Memory [%]                                                                           %                          82.31
    DRAM Throughput                                                                      %                          82.31
    Duration                                                                       usecond                          44.61
    L1/TEX Cache Throughput                                                              %                          30.51
    L2 Cache Throughput                                                                  %                          32.00
    SM Active Cycles                                                                 cycle                       50276.31
    Compute (SM) [%]                                                                     %                          15.25
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          81.21
    Achieved Active Warps Per SM                                                      warp                          51.98
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (81.2%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void reduce_dim_1<add_op>(device_tensor<(int)1>, device_tensor<(int)2>), 2022-Oct-10 02:10:15, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         876.85
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       10907858
    Memory [%]                                                                           %                           0.48
    DRAM Throughput                                                                      %                           0.22
    Duration                                                                       msecond                           8.32
    L1/TEX Cache Throughput                                                              %                          38.45
    L2 Cache Throughput                                                                  %                           0.08
    SM Active Cycles                                                                 cycle                      136351.73
    Compute (SM) [%]                                                                     %                           0.02
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             24
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             84
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<div_op, (int)1>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:15, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         558.04
    SM Frequency                                                             cycle/usecond                         846.26
    Elapsed Cycles                                                                   cycle                           3034
    Memory [%]                                                                           %                           1.60
    DRAM Throughput                                                                      %                           1.60
    Duration                                                                       usecond                           3.58
    L1/TEX Cache Throughput                                                              %                           1.06
    L2 Cache Throughput                                                                  %                           0.92
    SM Active Cycles                                                                 cycle                        1213.21
    Compute (SM) [%]                                                                     %                           0.40
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.38
    Achieved Active Warps Per SM                                                      warp                           1.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_fill_apply_v2_dim1<(int)1>(device_tensor<T1>, float), 2022-Oct-10 02:10:15, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         594.44
    SM Frequency                                                             cycle/usecond                         876.22
    Elapsed Cycles                                                                   cycle                           2524
    Memory [%]                                                                           %                           0.61
    DRAM Throughput                                                                      %                           0.01
    Duration                                                                       usecond                           2.88
    L1/TEX Cache Throughput                                                              %                           1.25
    L2 Cache Throughput                                                                  %                           0.61
    SM Active Cycles                                                                 cycle                         674.59
    Compute (SM) [%]                                                                     %                           0.24
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.29
    Achieved Active Warps Per SM                                                      warp                           1.46
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.3%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_broadcast_apply<sub_op>(device_tensor<(int)2>, device_tensor<(int)2>, device_tensor<(int)1>), 2022-Oct-10 02:10:16, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.56
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       24226051
    Memory [%]                                                                           %                           0.60
    DRAM Throughput                                                                      %                           0.21
    Duration                                                                       msecond                          18.45
    L1/TEX Cache Throughput                                                              %                          45.99
    L2 Cache Throughput                                                                  %                           0.60
    SM Active Cycles                                                                 cycle                      302738.16
    Compute (SM) [%]                                                                     %                           0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                           1.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<add_op, (int)1>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:16, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         575.22
    SM Frequency                                                             cycle/usecond                         859.47
    Elapsed Cycles                                                                   cycle                           3108
    Memory [%]                                                                           %                           1.54
    DRAM Throughput                                                                      %                           1.54
    Duration                                                                       usecond                           3.62
    L1/TEX Cache Throughput                                                              %                           1.05
    L2 Cache Throughput                                                                  %                           0.90
    SM Active Cycles                                                                 cycle                        1216.40
    Compute (SM) [%]                                                                     %                           0.26
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.37
    Achieved Active Warps Per SM                                                      warp                           1.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<square_root_op, (int)1>(device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:16, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         558.56
    SM Frequency                                                             cycle/usecond                         835.40
    Elapsed Cycles                                                                   cycle                           2968
    Memory [%]                                                                           %                           0.81
    DRAM Throughput                                                                      %                           0.81
    Duration                                                                       usecond                           3.55
    L1/TEX Cache Throughput                                                              %                           0.87
    L2 Cache Throughput                                                                  %                           0.67
    SM Active Cycles                                                                 cycle                        1155.30
    Compute (SM) [%]                                                                     %                           0.36
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.37
    Achieved Active Warps Per SM                                                      warp                           1.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_broadcast_apply<div_op>(device_tensor<(int)2>, device_tensor<(int)2>, device_tensor<(int)1>), 2022-Oct-10 02:10:17, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.53
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       28923612
    Memory [%]                                                                           %                           0.50
    DRAM Throughput                                                                      %                           0.18
    Duration                                                                       msecond                          22.03
    L1/TEX Cache Throughput                                                              %                          38.51
    L2 Cache Throughput                                                                  %                           0.50
    SM Active Cycles                                                                 cycle                      361524.97
    Compute (SM) [%]                                                                     %                           0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             35
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             48
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_fill_apply_v2_dim2<(int)2>(device_tensor<T1>, float), 2022-Oct-10 02:10:17, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         784.09
    SM Frequency                                                             cycle/nsecond                           1.17
    Elapsed Cycles                                                                   cycle                          23907
    Memory [%]                                                                           %                          79.03
    DRAM Throughput                                                                      %                          79.03
    Duration                                                                       usecond                          20.42
    L1/TEX Cache Throughput                                                              %                          54.91
    L2 Cache Throughput                                                                  %                          36.21
    SM Active Cycles                                                                 cycle                       20720.78
    Compute (SM) [%]                                                                     %                          27.45
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          52.09
    Achieved Active Warps Per SM                                                      warp                          33.34
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (52.1%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim2<div_op, (int)2>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:17, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         826.66
    SM Frequency                                                             cycle/nsecond                           1.23
    Elapsed Cycles                                                                   cycle                          76830
    Memory [%]                                                                           %                          89.51
    DRAM Throughput                                                                      %                          89.51
    Duration                                                                       usecond                          62.30
    L1/TEX Cache Throughput                                                              %                          25.61
    L2 Cache Throughput                                                                  %                          33.56
    SM Active Cycles                                                                 cycle                       73694.82
    Compute (SM) [%]                                                                     %                          15.54
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          85.28
    Achieved Active Warps Per SM                                                      warp                          54.58
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (85.3%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim2<sinh_op, (int)2>(device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:17, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         826.46
    SM Frequency                                                             cycle/nsecond                           1.23
    Elapsed Cycles                                                                   cycle                          55499
    Memory [%]                                                                           %                          79.81
    DRAM Throughput                                                                      %                          79.81
    Duration                                                                       usecond                          44.99
    L1/TEX Cache Throughput                                                              %                          29.55
    L2 Cache Throughput                                                                  %                          31.01
    SM Active Cycles                                                                 cycle                       52566.39
    Compute (SM) [%]                                                                     %                          31.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          78.00
    Achieved Active Warps Per SM                                                      warp                          49.92
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (78.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void reduce_dim_1<add_op>(device_tensor<(int)1>, device_tensor<(int)2>), 2022-Oct-10 02:10:17, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.35
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       10909813
    Memory [%]                                                                           %                           0.48
    DRAM Throughput                                                                      %                           0.22
    Duration                                                                       msecond                           8.31
    L1/TEX Cache Throughput                                                              %                          38.46
    L2 Cache Throughput                                                                  %                           0.08
    SM Active Cycles                                                                 cycle                      136325.44
    Compute (SM) [%]                                                                     %                           0.02
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             24
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             84
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                           1.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_fill_apply_v2_dim1<(int)1>(device_tensor<T1>, float), 2022-Oct-10 02:10:18, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         601.12
    SM Frequency                                                             cycle/usecond                         885.12
    Elapsed Cycles                                                                   cycle                           2521
    Memory [%]                                                                           %                           0.61
    DRAM Throughput                                                                      %                           0.01
    Duration                                                                       usecond                           2.85
    L1/TEX Cache Throughput                                                              %                           1.24
    L2 Cache Throughput                                                                  %                           0.61
    SM Active Cycles                                                                 cycle                         680.12
    Compute (SM) [%]                                                                     %                           0.24
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.29
    Achieved Active Warps Per SM                                                      warp                           1.47
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.3%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<div_op, (int)1>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:18, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         571.43
    SM Frequency                                                             cycle/usecond                         861.00
    Elapsed Cycles                                                                   cycle                           3087
    Memory [%]                                                                           %                           1.56
    DRAM Throughput                                                                      %                           1.56
    Duration                                                                       usecond                           3.58
    L1/TEX Cache Throughput                                                              %                           1.03
    L2 Cache Throughput                                                                  %                           0.91
    SM Active Cycles                                                                 cycle                        1242.31
    Compute (SM) [%]                                                                     %                           0.39
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.39
    Achieved Active Warps Per SM                                                      warp                           1.53
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_broadcast_apply<sub_op>(device_tensor<(int)2>, device_tensor<(int)2>, device_tensor<(int)1>), 2022-Oct-10 02:10:18, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.27
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       24222884
    Memory [%]                                                                           %                           0.60
    DRAM Throughput                                                                      %                           0.21
    Duration                                                                       msecond                          18.46
    L1/TEX Cache Throughput                                                              %                          45.99
    L2 Cache Throughput                                                                  %                           0.60
    SM Active Cycles                                                                 cycle                      302754.41
    Compute (SM) [%]                                                                     %                           0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                           1.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim2<square_op, (int)2>(device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:18, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         805.21
    SM Frequency                                                             cycle/nsecond                           1.20
    Elapsed Cycles                                                                   cycle                          53737
    Memory [%]                                                                           %                          82.40
    DRAM Throughput                                                                      %                          82.40
    Duration                                                                       usecond                          44.70
    L1/TEX Cache Throughput                                                              %                          30.52
    L2 Cache Throughput                                                                  %                          32.01
    SM Active Cycles                                                                 cycle                       50229.31
    Compute (SM) [%]                                                                     %                          15.26
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          81.46
    Achieved Active Warps Per SM                                                      warp                          52.14
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (81.5%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void reduce_dim_1<add_op>(device_tensor<(int)1>, device_tensor<(int)2>), 2022-Oct-10 02:10:19, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.06
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       10910756
    Memory [%]                                                                           %                           0.48
    DRAM Throughput                                                                      %                           0.22
    Duration                                                                       msecond                           8.32
    L1/TEX Cache Throughput                                                              %                          38.45
    L2 Cache Throughput                                                                  %                           0.08
    SM Active Cycles                                                                 cycle                      136369.67
    Compute (SM) [%]                                                                     %                           0.02
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             24
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             84
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<div_op, (int)1>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:19, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         566.37
    SM Frequency                                                             cycle/usecond                         845.13
    Elapsed Cycles                                                                   cycle                           3057
    Memory [%]                                                                           %                           1.56
    DRAM Throughput                                                                      %                           1.56
    Duration                                                                       usecond                           3.62
    L1/TEX Cache Throughput                                                              %                           0.94
    L2 Cache Throughput                                                                  %                           0.92
    SM Active Cycles                                                                 cycle                        1360.70
    Compute (SM) [%]                                                                     %                           0.39
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.39
    Achieved Active Warps Per SM                                                      warp                           1.53
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_fill_apply_v2_dim1<(int)1>(device_tensor<T1>, float), 2022-Oct-10 02:10:19, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         563.16
    SM Frequency                                                             cycle/usecond                         840.02
    Elapsed Cycles                                                                   cycle                           2555
    Memory [%]                                                                           %                           0.60
    DRAM Throughput                                                                      %                           0.01
    Duration                                                                       usecond                           3.04
    L1/TEX Cache Throughput                                                              %                           0.92
    L2 Cache Throughput                                                                  %                           0.60
    SM Active Cycles                                                                 cycle                         913.79
    Compute (SM) [%]                                                                     %                           0.23
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.34
    Achieved Active Warps Per SM                                                      warp                           1.50
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.3%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_broadcast_apply<sub_op>(device_tensor<(int)2>, device_tensor<(int)2>, device_tensor<(int)1>), 2022-Oct-10 02:10:19, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.36
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       24219336
    Memory [%]                                                                           %                           0.60
    DRAM Throughput                                                                      %                           0.21
    Duration                                                                       msecond                          18.45
    L1/TEX Cache Throughput                                                              %                          45.99
    L2 Cache Throughput                                                                  %                           0.60
    SM Active Cycles                                                                 cycle                      302747.80
    Compute (SM) [%]                                                                     %                           0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<add_op, (int)1>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:20, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         566.96
    SM Frequency                                                             cycle/usecond                         841.98
    Elapsed Cycles                                                                   cycle                           3019
    Memory [%]                                                                           %                           1.58
    DRAM Throughput                                                                      %                           1.58
    Duration                                                                       usecond                           3.58
    L1/TEX Cache Throughput                                                              %                           1.05
    L2 Cache Throughput                                                                  %                           0.93
    SM Active Cycles                                                                 cycle                        1215.03
    Compute (SM) [%]                                                                     %                           0.27
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.37
    Achieved Active Warps Per SM                                                      warp                           1.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<square_root_op, (int)1>(device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:20, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         558.56
    SM Frequency                                                             cycle/usecond                         844.88
    Elapsed Cycles                                                                   cycle                           3002
    Memory [%]                                                                           %                           0.81
    DRAM Throughput                                                                      %                           0.81
    Duration                                                                       usecond                           3.55
    L1/TEX Cache Throughput                                                              %                           0.86
    L2 Cache Throughput                                                                  %                           0.66
    SM Active Cycles                                                                 cycle                        1160.19
    Compute (SM) [%]                                                                     %                           0.36
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.37
    Achieved Active Warps Per SM                                                      warp                           1.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_broadcast_apply<div_op>(device_tensor<(int)2>, device_tensor<(int)2>, device_tensor<(int)1>), 2022-Oct-10 02:10:20, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.58
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       28926725
    Memory [%]                                                                           %                           0.50
    DRAM Throughput                                                                      %                           0.18
    Duration                                                                       msecond                          22.03
    L1/TEX Cache Throughput                                                              %                          38.51
    L2 Cache Throughput                                                                  %                           0.50
    SM Active Cycles                                                                 cycle                      361553.60
    Compute (SM) [%]                                                                     %                           0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             35
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             48
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_fill_apply_v2_dim2<(int)2>(device_tensor<T1>, float), 2022-Oct-10 02:10:20, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         778.56
    SM Frequency                                                             cycle/nsecond                           1.16
    Elapsed Cycles                                                                   cycle                          23743
    Memory [%]                                                                           %                          79.17
    DRAM Throughput                                                                      %                          79.17
    Duration                                                                       usecond                          20.45
    L1/TEX Cache Throughput                                                              %                          55.29
    L2 Cache Throughput                                                                  %                          36.49
    SM Active Cycles                                                                 cycle                       20786.54
    Compute (SM) [%]                                                                     %                          27.64
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          52.47
    Achieved Active Warps Per SM                                                      warp                          33.58
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (52.5%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim2<div_op, (int)2>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:21, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         828.30
    SM Frequency                                                             cycle/nsecond                           1.24
    Elapsed Cycles                                                                   cycle                          76683
    Memory [%]                                                                           %                          89.70
    DRAM Throughput                                                                      %                          89.70
    Duration                                                                       usecond                          62.02
    L1/TEX Cache Throughput                                                              %                          25.66
    L2 Cache Throughput                                                                  %                          33.63
    SM Active Cycles                                                                 cycle                       73959.61
    Compute (SM) [%]                                                                     %                          15.58
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          84.97
    Achieved Active Warps Per SM                                                      warp                          54.38
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (85.0%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim2<sinh_op, (int)2>(device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:21, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         820.32
    SM Frequency                                                             cycle/nsecond                           1.22
    Elapsed Cycles                                                                   cycle                          55732
    Memory [%]                                                                           %                          79.51
    DRAM Throughput                                                                      %                          79.51
    Duration                                                                       usecond                          45.50
    L1/TEX Cache Throughput                                                              %                          29.44
    L2 Cache Throughput                                                                  %                          30.88
    SM Active Cycles                                                                 cycle                       52960.22
    Compute (SM) [%]                                                                     %                          36.37
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          79.62
    Achieved Active Warps Per SM                                                      warp                          50.96
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (79.6%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void reduce_dim_1<add_op>(device_tensor<(int)1>, device_tensor<(int)2>), 2022-Oct-10 02:10:21, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         876.60
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       10906889
    Memory [%]                                                                           %                           0.48
    DRAM Throughput                                                                      %                           0.23
    Duration                                                                       msecond                           8.32
    L1/TEX Cache Throughput                                                              %                          38.45
    L2 Cache Throughput                                                                  %                           0.08
    SM Active Cycles                                                                 cycle                      136362.98
    Compute (SM) [%]                                                                     %                           0.02
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             24
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             84
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_fill_apply_v2_dim1<(int)1>(device_tensor<T1>, float), 2022-Oct-10 02:10:21, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         594.44
    SM Frequency                                                             cycle/usecond                         876.16
    Elapsed Cycles                                                                   cycle                           2524
    Memory [%]                                                                           %                           0.61
    DRAM Throughput                                                                      %                           0.01
    Duration                                                                       usecond                           2.88
    L1/TEX Cache Throughput                                                              %                           1.25
    L2 Cache Throughput                                                                  %                           0.61
    SM Active Cycles                                                                 cycle                         674.14
    Compute (SM) [%]                                                                     %                           0.24
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.29
    Achieved Active Warps Per SM                                                      warp                           1.46
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.3%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<div_op, (int)1>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:21, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         556.52
    SM Frequency                                                             cycle/usecond                         833.51
    Elapsed Cycles                                                                   cycle                           3068
    Memory [%]                                                                           %                           1.56
    DRAM Throughput                                                                      %                           1.56
    Duration                                                                       usecond                           3.68
    L1/TEX Cache Throughput                                                              %                           1.02
    L2 Cache Throughput                                                                  %                           0.91
    SM Active Cycles                                                                 cycle                        1259.34
    Compute (SM) [%]                                                                     %                           0.39
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.39
    Achieved Active Warps Per SM                                                      warp                           1.53
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_broadcast_apply<sub_op>(device_tensor<(int)2>, device_tensor<(int)2>, device_tensor<(int)1>), 2022-Oct-10 02:10:22, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.33
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       24221085
    Memory [%]                                                                           %                           0.60
    DRAM Throughput                                                                      %                           0.21
    Duration                                                                       msecond                          18.46
    L1/TEX Cache Throughput                                                              %                          45.98
    L2 Cache Throughput                                                                  %                           0.60
    SM Active Cycles                                                                 cycle                      302795.45
    Compute (SM) [%]                                                                     %                           0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim2<square_op, (int)2>(device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:22, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         803.99
    SM Frequency                                                             cycle/nsecond                           1.20
    Elapsed Cycles                                                                   cycle                          53952
    Memory [%]                                                                           %                          82.17
    DRAM Throughput                                                                      %                          82.17
    Duration                                                                       usecond                          44.90
    L1/TEX Cache Throughput                                                              %                          30.40
    L2 Cache Throughput                                                                  %                          31.89
    SM Active Cycles                                                                 cycle                       50364.50
    Compute (SM) [%]                                                                     %                          15.20
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          80.82
    Achieved Active Warps Per SM                                                      warp                          51.72
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (80.8%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void reduce_dim_1<add_op>(device_tensor<(int)1>, device_tensor<(int)2>), 2022-Oct-10 02:10:22, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         876.60
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       10909910
    Memory [%]                                                                           %                           0.48
    DRAM Throughput                                                                      %                           0.22
    Duration                                                                       msecond                           8.32
    L1/TEX Cache Throughput                                                              %                          38.45
    L2 Cache Throughput                                                                  %                           0.08
    SM Active Cycles                                                                 cycle                      136372.41
    Compute (SM) [%]                                                                     %                           0.02
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             24
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             84
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                           1.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<div_op, (int)1>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:23, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         589.29
    SM Frequency                                                             cycle/usecond                         873.84
    Elapsed Cycles                                                                   cycle                           3133
    Memory [%]                                                                           %                           1.52
    DRAM Throughput                                                                      %                           1.52
    Duration                                                                       usecond                           3.58
    L1/TEX Cache Throughput                                                              %                           1.05
    L2 Cache Throughput                                                                  %                           0.89
    SM Active Cycles                                                                 cycle                        1215.33
    Compute (SM) [%]                                                                     %                           0.38
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.37
    Achieved Active Warps Per SM                                                      warp                           1.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_fill_apply_v2_dim1<(int)1>(device_tensor<T1>, float), 2022-Oct-10 02:10:23, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         583.33
    SM Frequency                                                             cycle/usecond                         883.68
    Elapsed Cycles                                                                   cycle                           2546
    Memory [%]                                                                           %                           0.61
    DRAM Throughput                                                                      %                           0.01
    Duration                                                                       usecond                           2.88
    L1/TEX Cache Throughput                                                              %                           0.97
    L2 Cache Throughput                                                                  %                           0.61
    SM Active Cycles                                                                 cycle                         861.66
    Compute (SM) [%]                                                                     %                           0.24
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.33
    Achieved Active Warps Per SM                                                      warp                           1.49
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.3%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_broadcast_apply<sub_op>(device_tensor<(int)2>, device_tensor<(int)2>, device_tensor<(int)1>), 2022-Oct-10 02:10:23, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.29
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       24220845
    Memory [%]                                                                           %                           0.60
    DRAM Throughput                                                                      %                           0.21
    Duration                                                                       msecond                          18.46
    L1/TEX Cache Throughput                                                              %                          45.99
    L2 Cache Throughput                                                                  %                           0.60
    SM Active Cycles                                                                 cycle                      302727.30
    Compute (SM) [%]                                                                     %                           0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<add_op, (int)1>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:23, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         589.29
    SM Frequency                                                             cycle/usecond                         870.49
    Elapsed Cycles                                                                   cycle                           3122
    Memory [%]                                                                           %                           1.52
    DRAM Throughput                                                                      %                           1.52
    Duration                                                                       usecond                           3.58
    L1/TEX Cache Throughput                                                              %                           0.91
    L2 Cache Throughput                                                                  %                           0.90
    SM Active Cycles                                                                 cycle                        1413.30
    Compute (SM) [%]                                                                     %                           0.26
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.38
    Achieved Active Warps Per SM                                                      warp                           1.53
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<square_root_op, (int)1>(device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:24, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         563.06
    SM Frequency                                                             cycle/usecond                         844.03
    Elapsed Cycles                                                                   cycle                           2999
    Memory [%]                                                                           %                           0.80
    DRAM Throughput                                                                      %                           0.80
    Duration                                                                       usecond                           3.55
    L1/TEX Cache Throughput                                                              %                           0.86
    L2 Cache Throughput                                                                  %                           0.66
    SM Active Cycles                                                                 cycle                        1156.86
    Compute (SM) [%]                                                                     %                           0.36
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.37
    Achieved Active Warps Per SM                                                      warp                           1.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_broadcast_apply<div_op>(device_tensor<(int)2>, device_tensor<(int)2>, device_tensor<(int)1>), 2022-Oct-10 02:10:24, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.65
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       28933568
    Memory [%]                                                                           %                           0.50
    DRAM Throughput                                                                      %                           0.18
    Duration                                                                       msecond                          22.04
    L1/TEX Cache Throughput                                                              %                          38.50
    L2 Cache Throughput                                                                  %                           0.50
    SM Active Cycles                                                                 cycle                      361626.06
    Compute (SM) [%]                                                                     %                           0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             35
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             48
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                           1.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_fill_apply_v2_dim2<(int)2>(device_tensor<T1>, float), 2022-Oct-10 02:10:24, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         779.30
    SM Frequency                                                             cycle/nsecond                           1.16
    Elapsed Cycles                                                                   cycle                          23774
    Memory [%]                                                                           %                          79.21
    DRAM Throughput                                                                      %                          79.21
    Duration                                                                       usecond                          20.48
    L1/TEX Cache Throughput                                                              %                          55.23
    L2 Cache Throughput                                                                  %                          36.43
    SM Active Cycles                                                                 cycle                       20662.06
    Compute (SM) [%]                                                                     %                          27.61
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          52.22
    Achieved Active Warps Per SM                                                      warp                          33.42
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (52.2%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim2<div_op, (int)2>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:24, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         829.85
    SM Frequency                                                             cycle/nsecond                           1.24
    Elapsed Cycles                                                                   cycle                          77664
    Memory [%]                                                                           %                          88.56
    DRAM Throughput                                                                      %                          88.56
    Duration                                                                       usecond                          62.72
    L1/TEX Cache Throughput                                                              %                          25.33
    L2 Cache Throughput                                                                  %                          33.20
    SM Active Cycles                                                                 cycle                       73946.35
    Compute (SM) [%]                                                                     %                          15.39
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          85.28
    Achieved Active Warps Per SM                                                      warp                          54.58
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (85.3%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim2<sinh_op, (int)2>(device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:25, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         828.03
    SM Frequency                                                             cycle/nsecond                           1.24
    Elapsed Cycles                                                                   cycle                          55939
    Memory [%]                                                                           %                          79.27
    DRAM Throughput                                                                      %                          79.27
    Duration                                                                       usecond                          45.22
    L1/TEX Cache Throughput                                                              %                          29.32
    L2 Cache Throughput                                                                  %                          30.77
    SM Active Cycles                                                                 cycle                       53204.25
    Compute (SM) [%]                                                                     %                          37.82
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          80.36
    Achieved Active Warps Per SM                                                      warp                          51.43
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (80.4%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void reduce_dim_1<add_op>(device_tensor<(int)1>, device_tensor<(int)2>), 2022-Oct-10 02:10:25, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.25
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       10912183
    Memory [%]                                                                           %                           0.48
    DRAM Throughput                                                                      %                           0.22
    Duration                                                                       msecond                           8.32
    L1/TEX Cache Throughput                                                              %                          38.45
    L2 Cache Throughput                                                                  %                           0.08
    SM Active Cycles                                                                 cycle                      136351.79
    Compute (SM) [%]                                                                     %                           0.02
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             24
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             84
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                           1.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_fill_apply_v2_dim1<(int)1>(device_tensor<T1>, float), 2022-Oct-10 02:10:25, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         605.34
    SM Frequency                                                             cycle/usecond                         893.61
    Elapsed Cycles                                                                   cycle                           2546
    Memory [%]                                                                           %                           0.61
    DRAM Throughput                                                                      %                           0.01
    Duration                                                                       usecond                           2.85
    L1/TEX Cache Throughput                                                              %                           1.24
    L2 Cache Throughput                                                                  %                           0.61
    SM Active Cycles                                                                 cycle                         677.67
    Compute (SM) [%]                                                                     %                           0.24
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.29
    Achieved Active Warps Per SM                                                      warp                           1.46
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.3%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<div_op, (int)1>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:25, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         566.37
    SM Frequency                                                             cycle/usecond                         846.19
    Elapsed Cycles                                                                   cycle                           3061
    Memory [%]                                                                           %                           1.56
    DRAM Throughput                                                                      %                           1.56
    Duration                                                                       usecond                           3.62
    L1/TEX Cache Throughput                                                              %                           0.92
    L2 Cache Throughput                                                                  %                           0.91
    SM Active Cycles                                                                 cycle                        1385.10
    Compute (SM) [%]                                                                     %                           0.39
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.39
    Achieved Active Warps Per SM                                                      warp                           1.53
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_broadcast_apply<sub_op>(device_tensor<(int)2>, device_tensor<(int)2>, device_tensor<(int)1>), 2022-Oct-10 02:10:26, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.20
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       24224054
    Memory [%]                                                                           %                           0.60
    DRAM Throughput                                                                      %                           0.21
    Duration                                                                       msecond                          18.46
    L1/TEX Cache Throughput                                                              %                          45.99
    L2 Cache Throughput                                                                  %                           0.60
    SM Active Cycles                                                                 cycle                      302745.34
    Compute (SM) [%]                                                                     %                           0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                           1.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim2<square_op, (int)2>(device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:26, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         812.99
    SM Frequency                                                             cycle/nsecond                           1.21
    Elapsed Cycles                                                                   cycle                          54151
    Memory [%]                                                                           %                          81.83
    DRAM Throughput                                                                      %                          81.83
    Duration                                                                       usecond                          44.58
    L1/TEX Cache Throughput                                                              %                          30.28
    L2 Cache Throughput                                                                  %                          31.76
    SM Active Cycles                                                                 cycle                       50600.68
    Compute (SM) [%]                                                                     %                          15.14
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          80.95
    Achieved Active Warps Per SM                                                      warp                          51.81
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (80.9%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void reduce_dim_1<add_op>(device_tensor<(int)1>, device_tensor<(int)2>), 2022-Oct-10 02:10:26, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         876.79
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       10910187
    Memory [%]                                                                           %                           0.48
    DRAM Throughput                                                                      %                           0.22
    Duration                                                                       msecond                           8.32
    L1/TEX Cache Throughput                                                              %                          38.45
    L2 Cache Throughput                                                                  %                           0.08
    SM Active Cycles                                                                 cycle                      136371.17
    Compute (SM) [%]                                                                     %                           0.02
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             24
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             84
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<div_op, (int)1>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:26, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         551.72
    SM Frequency                                                             cycle/usecond                         816.68
    Elapsed Cycles                                                                   cycle                           3033
    Memory [%]                                                                           %                           1.56
    DRAM Throughput                                                                      %                           1.56
    Duration                                                                       usecond                           3.71
    L1/TEX Cache Throughput                                                              %                           1.06
    L2 Cache Throughput                                                                  %                           0.92
    SM Active Cycles                                                                 cycle                        1210.75
    Compute (SM) [%]                                                                     %                           0.40
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.38
    Achieved Active Warps Per SM                                                      warp                           1.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_fill_apply_v2_dim1<(int)1>(device_tensor<T1>, float), 2022-Oct-10 02:10:26, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         590.28
    SM Frequency                                                             cycle/usecond                         876.56
    Elapsed Cycles                                                                   cycle                           2526
    Memory [%]                                                                           %                           0.61
    DRAM Throughput                                                                      %                           0.01
    Duration                                                                       usecond                           2.88
    L1/TEX Cache Throughput                                                              %                           1.24
    L2 Cache Throughput                                                                  %                           0.61
    SM Active Cycles                                                                 cycle                         678.05
    Compute (SM) [%]                                                                     %                           0.24
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.29
    Achieved Active Warps Per SM                                                      warp                           1.47
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.3%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_broadcast_apply<sub_op>(device_tensor<(int)2>, device_tensor<(int)2>, device_tensor<(int)1>), 2022-Oct-10 02:10:27, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.53
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       24225535
    Memory [%]                                                                           %                           0.60
    DRAM Throughput                                                                      %                           0.21
    Duration                                                                       msecond                          18.45
    L1/TEX Cache Throughput                                                              %                          46.00
    L2 Cache Throughput                                                                  %                           0.60
    SM Active Cycles                                                                 cycle                      302708.99
    Compute (SM) [%]                                                                     %                           0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                           1.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<add_op, (int)1>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:27, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         564.73
    SM Frequency                                                             cycle/usecond                         838.59
    Elapsed Cycles                                                                   cycle                           3006
    Memory [%]                                                                           %                           1.58
    DRAM Throughput                                                                      %                           1.58
    Duration                                                                       usecond                           3.58
    L1/TEX Cache Throughput                                                              %                           1.01
    L2 Cache Throughput                                                                  %                           0.93
    SM Active Cycles                                                                 cycle                        1261.46
    Compute (SM) [%]                                                                     %                           0.27
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.37
    Achieved Active Warps Per SM                                                      warp                           1.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<square_root_op, (int)1>(device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:27, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         558.56
    SM Frequency                                                             cycle/usecond                         838.21
    Elapsed Cycles                                                                   cycle                           2979
    Memory [%]                                                                           %                           0.81
    DRAM Throughput                                                                      %                           0.81
    Duration                                                                       usecond                           3.55
    L1/TEX Cache Throughput                                                              %                           0.86
    L2 Cache Throughput                                                                  %                           0.67
    SM Active Cycles                                                                 cycle                        1159.36
    Compute (SM) [%]                                                                     %                           0.36
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.37
    Achieved Active Warps Per SM                                                      warp                           1.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_broadcast_apply<div_op>(device_tensor<(int)2>, device_tensor<(int)2>, device_tensor<(int)1>), 2022-Oct-10 02:10:28, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.49
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       28931188
    Memory [%]                                                                           %                           0.50
    DRAM Throughput                                                                      %                           0.18
    Duration                                                                       msecond                          22.04
    L1/TEX Cache Throughput                                                              %                          38.50
    L2 Cache Throughput                                                                  %                           0.50
    SM Active Cycles                                                                 cycle                      361602.11
    Compute (SM) [%]                                                                     %                           0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             35
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             48
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                           1.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_fill_apply_v2_dim2<(int)2>(device_tensor<T1>, float), 2022-Oct-10 02:10:28, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         786.16
    SM Frequency                                                             cycle/nsecond                           1.17
    Elapsed Cycles                                                                   cycle                          23846
    Memory [%]                                                                           %                          79.07
    DRAM Throughput                                                                      %                          79.07
    Duration                                                                       usecond                          20.35
    L1/TEX Cache Throughput                                                              %                          55.05
    L2 Cache Throughput                                                                  %                          36.30
    SM Active Cycles                                                                 cycle                       20730.15
    Compute (SM) [%]                                                                     %                          27.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          52.26
    Achieved Active Warps Per SM                                                      warp                          33.45
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (52.3%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim2<div_op, (int)2>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:28, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         830.24
    SM Frequency                                                             cycle/nsecond                           1.24
    Elapsed Cycles                                                                   cycle                          76952
    Memory [%]                                                                           %                          89.41
    DRAM Throughput                                                                      %                          89.41
    Duration                                                                       usecond                          62.11
    L1/TEX Cache Throughput                                                              %                          25.57
    L2 Cache Throughput                                                                  %                          33.50
    SM Active Cycles                                                                 cycle                       73912.61
    Compute (SM) [%]                                                                     %                          15.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          85.26
    Achieved Active Warps Per SM                                                      warp                          54.57
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (85.3%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim2<sinh_op, (int)2>(device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:28, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         826.03
    SM Frequency                                                             cycle/nsecond                           1.23
    Elapsed Cycles                                                                   cycle                          55855
    Memory [%]                                                                           %                          79.38
    DRAM Throughput                                                                      %                          79.38
    Duration                                                                       usecond                          45.25
    L1/TEX Cache Throughput                                                              %                          29.37
    L2 Cache Throughput                                                                  %                          30.81
    SM Active Cycles                                                                 cycle                       53205.24
    Compute (SM) [%]                                                                     %                          38.42
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          80.61
    Achieved Active Warps Per SM                                                      warp                          51.59
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (80.6%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void reduce_dim_1<add_op>(device_tensor<(int)1>, device_tensor<(int)2>), 2022-Oct-10 02:10:29, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         876.97
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       10909300
    Memory [%]                                                                           %                           0.48
    DRAM Throughput                                                                      %                           0.23
    Duration                                                                       msecond                           8.32
    L1/TEX Cache Throughput                                                              %                          38.46
    L2 Cache Throughput                                                                  %                           0.08
    SM Active Cycles                                                                 cycle                      136341.16
    Compute (SM) [%]                                                                     %                           0.02
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             24
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             84
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                           1.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_fill_apply_v2_dim1<(int)1>(device_tensor<T1>, float), 2022-Oct-10 02:10:29, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         601.12
    SM Frequency                                                             cycle/usecond                         892.73
    Elapsed Cycles                                                                   cycle                           2543
    Memory [%]                                                                           %                           0.61
    DRAM Throughput                                                                      %                           0.01
    Duration                                                                       usecond                           2.85
    L1/TEX Cache Throughput                                                              %                           1.25
    L2 Cache Throughput                                                                  %                           0.61
    SM Active Cycles                                                                 cycle                         673.27
    Compute (SM) [%]                                                                     %                           0.24
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.29
    Achieved Active Warps Per SM                                                      warp                           1.46
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.3%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<div_op, (int)1>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:29, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         561.40
    SM Frequency                                                             cycle/usecond                         846.17
    Elapsed Cycles                                                                   cycle                           3088
    Memory [%]                                                                           %                           1.56
    DRAM Throughput                                                                      %                           1.56
    Duration                                                                       usecond                           3.65
    L1/TEX Cache Throughput                                                              %                           1.03
    L2 Cache Throughput                                                                  %                           0.91
    SM Active Cycles                                                                 cycle                        1248.15
    Compute (SM) [%]                                                                     %                           0.39
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.38
    Achieved Active Warps Per SM                                                      warp                           1.53
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_broadcast_apply<sub_op>(device_tensor<(int)2>, device_tensor<(int)2>, device_tensor<(int)1>), 2022-Oct-10 02:10:29, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.17
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       24220378
    Memory [%]                                                                           %                           0.60
    DRAM Throughput                                                                      %                           0.21
    Duration                                                                       msecond                          18.46
    L1/TEX Cache Throughput                                                              %                          45.99
    L2 Cache Throughput                                                                  %                           0.60
    SM Active Cycles                                                                 cycle                      302739.10
    Compute (SM) [%]                                                                     %                           0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim2<square_op, (int)2>(device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:30, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         804.29
    SM Frequency                                                             cycle/nsecond                           1.20
    Elapsed Cycles                                                                   cycle                          53816
    Memory [%]                                                                           %                          82.31
    DRAM Throughput                                                                      %                          82.31
    Duration                                                                       usecond                          44.80
    L1/TEX Cache Throughput                                                              %                          30.48
    L2 Cache Throughput                                                                  %                          31.96
    SM Active Cycles                                                                 cycle                       50505.28
    Compute (SM) [%]                                                                     %                          15.24
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          80.91
    Achieved Active Warps Per SM                                                      warp                          51.79
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (80.9%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void reduce_dim_1<add_op>(device_tensor<(int)1>, device_tensor<(int)2>), 2022-Oct-10 02:10:30, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         876.55
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       10906925
    Memory [%]                                                                           %                           0.48
    DRAM Throughput                                                                      %                           0.22
    Duration                                                                       msecond                           8.32
    L1/TEX Cache Throughput                                                              %                          38.46
    L2 Cache Throughput                                                                  %                           0.08
    SM Active Cycles                                                                 cycle                      136341.20
    Compute (SM) [%]                                                                     %                           0.02
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             24
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             84
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<div_op, (int)1>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:30, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         557.02
    SM Frequency                                                             cycle/usecond                         829.22
    Elapsed Cycles                                                                   cycle                           3026
    Memory [%]                                                                           %                           1.58
    DRAM Throughput                                                                      %                           1.58
    Duration                                                                       usecond                           3.65
    L1/TEX Cache Throughput                                                              %                           1.05
    L2 Cache Throughput                                                                  %                           0.92
    SM Active Cycles                                                                 cycle                        1217.35
    Compute (SM) [%]                                                                     %                           0.40
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.37
    Achieved Active Warps Per SM                                                      warp                           1.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_fill_apply_v2_dim1<(int)1>(device_tensor<T1>, float), 2022-Oct-10 02:10:30, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         583.33
    SM Frequency                                                             cycle/usecond                         874.54
    Elapsed Cycles                                                                   cycle                           2520
    Memory [%]                                                                           %                           0.61
    DRAM Throughput                                                                      %                           0.01
    Duration                                                                       usecond                           2.88
    L1/TEX Cache Throughput                                                              %                           1.24
    L2 Cache Throughput                                                                  %                           0.61
    SM Active Cycles                                                                 cycle                         676.71
    Compute (SM) [%]                                                                     %                           0.24
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.29
    Achieved Active Warps Per SM                                                      warp                           1.46
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.3%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_broadcast_apply<sub_op>(device_tensor<(int)2>, device_tensor<(int)2>, device_tensor<(int)1>), 2022-Oct-10 02:10:31, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.09
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       24214528
    Memory [%]                                                                           %                           0.60
    DRAM Throughput                                                                      %                           0.21
    Duration                                                                       msecond                          18.46
    L1/TEX Cache Throughput                                                              %                          45.99
    L2 Cache Throughput                                                                  %                           0.60
    SM Active Cycles                                                                 cycle                      302718.78
    Compute (SM) [%]                                                                     %                           0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<add_op, (int)1>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:31, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         575.89
    SM Frequency                                                             cycle/usecond                         867.33
    Elapsed Cycles                                                                   cycle                           3109
    Memory [%]                                                                           %                           1.55
    DRAM Throughput                                                                      %                           1.55
    Duration                                                                       usecond                           3.58
    L1/TEX Cache Throughput                                                              %                           1.05
    L2 Cache Throughput                                                                  %                           0.90
    SM Active Cycles                                                                 cycle                        1217.54
    Compute (SM) [%]                                                                     %                           0.26
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.37
    Achieved Active Warps Per SM                                                      warp                           1.51
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<square_root_op, (int)1>(device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:31, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         553.57
    SM Frequency                                                             cycle/usecond                         828.12
    Elapsed Cycles                                                                   cycle                           2969
    Memory [%]                                                                           %                           0.81
    DRAM Throughput                                                                      %                           0.81
    Duration                                                                       usecond                           3.58
    L1/TEX Cache Throughput                                                              %                           0.87
    L2 Cache Throughput                                                                  %                           0.67
    SM Active Cycles                                                                 cycle                           1155
    Compute (SM) [%]                                                                     %                           0.36
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.37
    Achieved Active Warps Per SM                                                      warp                           1.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_broadcast_apply<div_op>(device_tensor<(int)2>, device_tensor<(int)2>, device_tensor<(int)1>), 2022-Oct-10 02:10:32, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.64
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       28930871
    Memory [%]                                                                           %                           0.50
    DRAM Throughput                                                                      %                           0.18
    Duration                                                                       msecond                          22.04
    L1/TEX Cache Throughput                                                              %                          38.51
    L2 Cache Throughput                                                                  %                           0.50
    SM Active Cycles                                                                 cycle                      361581.66
    Compute (SM) [%]                                                                     %                           0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             35
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             48
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                           1.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_fill_apply_v2_dim2<(int)2>(device_tensor<T1>, float), 2022-Oct-10 02:10:32, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         773.72
    SM Frequency                                                             cycle/nsecond                           1.15
    Elapsed Cycles                                                                   cycle                          23749
    Memory [%]                                                                           %                          79.57
    DRAM Throughput                                                                      %                          79.57
    Duration                                                                       usecond                          20.58
    L1/TEX Cache Throughput                                                              %                          55.28
    L2 Cache Throughput                                                                  %                          36.41
    SM Active Cycles                                                                 cycle                       20766.12
    Compute (SM) [%]                                                                     %                          27.63
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          52.34
    Achieved Active Warps Per SM                                                      warp                          33.50
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (52.3%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim2<div_op, (int)2>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:32, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         832.39
    SM Frequency                                                             cycle/nsecond                           1.24
    Elapsed Cycles                                                                   cycle                          77046
    Memory [%]                                                                           %                          89.29
    DRAM Throughput                                                                      %                          89.29
    Duration                                                                       usecond                          62.05
    L1/TEX Cache Throughput                                                              %                          25.54
    L2 Cache Throughput                                                                  %                          33.47
    SM Active Cycles                                                                 cycle                       73795.36
    Compute (SM) [%]                                                                     %                          15.50
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          85.30
    Achieved Active Warps Per SM                                                      warp                          54.59
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (85.3%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim2<sinh_op, (int)2>(device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:32, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         827.59
    SM Frequency                                                             cycle/nsecond                           1.23
    Elapsed Cycles                                                                   cycle                          56159
    Memory [%]                                                                           %                          78.88
    DRAM Throughput                                                                      %                          78.88
    Duration                                                                       usecond                          45.47
    L1/TEX Cache Throughput                                                              %                          29.21
    L2 Cache Throughput                                                                  %                          30.64
    SM Active Cycles                                                                 cycle                       53307.35
    Compute (SM) [%]                                                                     %                          38.30
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          80.71
    Achieved Active Warps Per SM                                                      warp                          51.65
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (80.7%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void reduce_dim_1<add_op>(device_tensor<(int)1>, device_tensor<(int)2>), 2022-Oct-10 02:10:32, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.09
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       10913242
    Memory [%]                                                                           %                           0.48
    DRAM Throughput                                                                      %                           0.22
    Duration                                                                       msecond                           8.32
    L1/TEX Cache Throughput                                                              %                          38.46
    L2 Cache Throughput                                                                  %                           0.08
    SM Active Cycles                                                                 cycle                      136320.20
    Compute (SM) [%]                                                                     %                           0.02
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             24
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             84
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_fill_apply_v2_dim1<(int)1>(device_tensor<T1>, float), 2022-Oct-10 02:10:32, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         601.12
    SM Frequency                                                             cycle/usecond                         887.70
    Elapsed Cycles                                                                   cycle                           2529
    Memory [%]                                                                           %                           0.61
    DRAM Throughput                                                                      %                           0.01
    Duration                                                                       usecond                           2.85
    L1/TEX Cache Throughput                                                              %                           1.24
    L2 Cache Throughput                                                                  %                           0.61
    SM Active Cycles                                                                 cycle                         676.67
    Compute (SM) [%]                                                                     %                           0.24
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.29
    Achieved Active Warps Per SM                                                      warp                           1.47
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.3%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<div_op, (int)1>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:33, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         571.90
    SM Frequency                                                             cycle/usecond                         853.71
    Elapsed Cycles                                                                   cycle                           3088
    Memory [%]                                                                           %                           1.55
    DRAM Throughput                                                                      %                           1.55
    Duration                                                                       usecond                           3.62
    L1/TEX Cache Throughput                                                              %                           1.03
    L2 Cache Throughput                                                                  %                           0.91
    SM Active Cycles                                                                 cycle                        1242.71
    Compute (SM) [%]                                                                     %                           0.39
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.39
    Achieved Active Warps Per SM                                                      warp                           1.53
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_broadcast_apply<sub_op>(device_tensor<(int)2>, device_tensor<(int)2>, device_tensor<(int)1>), 2022-Oct-10 02:10:33, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.06
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       24218436
    Memory [%]                                                                           %                           0.60
    DRAM Throughput                                                                      %                           0.21
    Duration                                                                       msecond                          18.46
    L1/TEX Cache Throughput                                                              %                          45.99
    L2 Cache Throughput                                                                  %                           0.60
    SM Active Cycles                                                                 cycle                      302719.69
    Compute (SM) [%]                                                                     %                           0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                           1.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim2<square_op, (int)2>(device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:33, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         799.50
    SM Frequency                                                             cycle/nsecond                           1.19
    Elapsed Cycles                                                                   cycle                          54019
    Memory [%]                                                                           %                          81.96
    DRAM Throughput                                                                      %                          81.96
    Duration                                                                       usecond                          45.25
    L1/TEX Cache Throughput                                                              %                          30.36
    L2 Cache Throughput                                                                  %                          31.84
    SM Active Cycles                                                                 cycle                       50333.04
    Compute (SM) [%]                                                                     %                          15.18
    ---------------------------------------------------------------------- --------------- ------------------------------
    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                       1024
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                        4096
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                        4194304
    Waves Per SM                                                                                                    25.60
    ---------------------------------------------------------------------- --------------- ------------------------------

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                              4
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                              2
    Theoretical Active Warps per SM                                                   warp                             64
    Theoretical Occupancy                                                                %                            100
    Achieved Occupancy                                                                   %                          80.68
    Achieved Active Warps Per SM                                                      warp                          51.64
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     
          theoretical (100.0%) and measured achieved occupancy (80.7%) can be the result of warp scheduling overheads   
          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    
          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void reduce_dim_1<add_op>(device_tensor<(int)1>, device_tensor<(int)2>), 2022-Oct-10 02:10:34, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         876.53
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       10907674
    Memory [%]                                                                           %                           0.48
    DRAM Throughput                                                                      %                           0.22
    Duration                                                                       msecond                           8.32
    L1/TEX Cache Throughput                                                              %                          38.48
    L2 Cache Throughput                                                                  %                           0.08
    SM Active Cycles                                                                 cycle                      136275.39
    Compute (SM) [%]                                                                     %                           0.02
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             24
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             84
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                           1.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<div_op, (int)1>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:34, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         564.69
    SM Frequency                                                             cycle/usecond                         851.84
    Elapsed Cycles                                                                   cycle                           3109
    Memory [%]                                                                           %                           1.55
    DRAM Throughput                                                                      %                           1.55
    Duration                                                                       usecond                           3.65
    L1/TEX Cache Throughput                                                              %                           1.06
    L2 Cache Throughput                                                                  %                           0.90
    SM Active Cycles                                                                 cycle                        1211.51
    Compute (SM) [%]                                                                     %                           0.39
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.37
    Achieved Active Warps Per SM                                                      warp                           1.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_fill_apply_v2_dim1<(int)1>(device_tensor<T1>, float), 2022-Oct-10 02:10:34, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         588.89
    SM Frequency                                                             cycle/usecond                         884.66
    Elapsed Cycles                                                                   cycle                           2549
    Memory [%]                                                                           %                           0.61
    DRAM Throughput                                                                      %                           0.01
    Duration                                                                       usecond                           2.88
    L1/TEX Cache Throughput                                                              %                           1.24
    L2 Cache Throughput                                                                  %                           0.61
    SM Active Cycles                                                                 cycle                         678.06
    Compute (SM) [%]                                                                     %                           0.24
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.29
    Achieved Active Warps Per SM                                                      warp                           1.46
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.3%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_broadcast_apply<sub_op>(device_tensor<(int)2>, device_tensor<(int)2>, device_tensor<(int)1>), 2022-Oct-10 02:10:34, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.59
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       24223363
    Memory [%]                                                                           %                           0.60
    DRAM Throughput                                                                      %                           0.21
    Duration                                                                       msecond                          18.45
    L1/TEX Cache Throughput                                                              %                          46.00
    L2 Cache Throughput                                                                  %                           0.60
    SM Active Cycles                                                                 cycle                      302711.15
    Compute (SM) [%]                                                                     %                           0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             32
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             64
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<add_op, (int)1>(device_tensor<T2>, device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:35, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         591.52
    SM Frequency                                                             cycle/usecond                         882.67
    Elapsed Cycles                                                                   cycle                           3165
    Memory [%]                                                                           %                           1.51
    DRAM Throughput                                                                      %                           1.51
    Duration                                                                       usecond                           3.58
    L1/TEX Cache Throughput                                                              %                           1.06
    L2 Cache Throughput                                                                  %                           0.88
    SM Active Cycles                                                                 cycle                        1208.56
    Compute (SM) [%]                                                                     %                           0.25
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.37
    Achieved Active Warps Per SM                                                      warp                           1.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_pointwise_apply_v2_dim1<square_root_op, (int)1>(device_tensor<T2>, device_tensor<T2>), 2022-Oct-10 02:10:35, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         567.57
    SM Frequency                                                             cycle/usecond                         847.60
    Elapsed Cycles                                                                   cycle                           3012
    Memory [%]                                                                           %                           0.80
    DRAM Throughput                                                                      %                           0.80
    Duration                                                                       usecond                           3.55
    L1/TEX Cache Throughput                                                              %                           0.86
    L2 Cache Throughput                                                                  %                           0.66
    SM Active Cycles                                                                 cycle                        1161.91
    Compute (SM) [%]                                                                     %                           0.36
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                         128
    Registers Per Thread                                                   register/thread                             16
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                           4096
    Waves Per SM                                                                                                     0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                            128
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           2.37
    Achieved Active Warps Per SM                                                      warp                           1.52
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (2.4%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

  void kernel_broadcast_apply<div_op>(device_tensor<(int)2>, device_tensor<(int)2>, device_tensor<(int)1>), 2022-Oct-10 02:10:35, Context 1, Stream 7
    Section: GPU Speed Of Light Throughput
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                           cycle/usecond                         877.63
    SM Frequency                                                             cycle/nsecond                           1.31
    Elapsed Cycles                                                                   cycle                       28929493
    Memory [%]                                                                           %                           0.50
    DRAM Throughput                                                                      %                           0.18
    Duration                                                                       msecond                          22.04
    L1/TEX Cache Throughput                                                              %                          38.51
    L2 Cache Throughput                                                                  %                           0.50
    SM Active Cycles                                                                 cycle                      361588.05
    Compute (SM) [%]                                                                     %                           0.05
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                         32
    Function Cache Configuration                                                                  cudaFuncCachePreferNone
    Grid Size                                                                                                           1
    Registers Per Thread                                                   register/thread                             35
    Shared Memory Configuration Size                                                  byte                              0
    Driver Shared Memory Per Block                                              byte/block                              0
    Dynamic Shared Memory Per Block                                             byte/block                              0
    Static Shared Memory Per Block                                              byte/block                              0
    Threads                                                                         thread                             32
    Waves Per SM                                                                                                     0.00
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                   block                             32
    Block Limit Registers                                                            block                             48
    Block Limit Shared Mem                                                           block                             32
    Block Limit Warps                                                                block                             64
    Theoretical Active Warps per SM                                                   warp                             32
    Theoretical Occupancy                                                                %                             50
    Achieved Occupancy                                                                   %                           1.56
    Achieved Active Warps Per SM                                                      warp                              1
    ---------------------------------------------------------------------- --------------- ------------------------------
    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM This    
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory The difference      
          between calculated theoretical (50.0%) and measured achieved occupancy (1.6%) can be the result of warp       
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

